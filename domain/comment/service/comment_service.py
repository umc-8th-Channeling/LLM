import random
from collections import defaultdict
from typing import List, DefaultDict
import logging
import time
from domain.comment.model.comment import Comment
from domain.comment.model.comment_type import CommentType
from domain.comment.repository.comment_repository import CommentRepository
from domain.report.repository.report_repository import ReportRepository
from domain.video.model.video import Video
from external.rag.rag_service_impl import RagServiceImpl
from external.youtube.youtube_comment_service import YoutubeCommentService

logger = logging.getLogger(__name__)

class CommentService:
    def __init__(self):
        self.rag_service = RagServiceImpl()
        self.comment_repository = CommentRepository()
        self.youtube_comment_service = YoutubeCommentService()
        self.report_repository = ReportRepository()

    async def summarize_comments_by_emotions_with_llm(self, comments_by_emotions: DefaultDict[CommentType, list[Comment]]) -> defaultdict[CommentType, List[Comment]]:
        summarized_comments: defaultdict[CommentType, List[Comment]] = defaultdict(list)

        summarize_and_save_start = time.time()
        logger.info("ğŸ“ ëŒ“ê¸€ ê°ì •ë³„ ìš”ì•½ ë° ì €ì¥ ì‹œì‘")
        # ê°ì •ë³„ë¡œ ìš”ì•½
        for emotion, comments in comments_by_emotions.items():
            if not comments:
                continue

            # í•´ë‹¹ ê°ì • ê·¸ë£¹ì˜ contentë§Œ ê°œí–‰ìœ¼ë¡œ í•©ì¹˜ê¸°
            contents_str = "\n".join(comment.content for comment in comments)

            # LLM ì„œë¹„ìŠ¤ í˜¸ì¶œ -> returns list[str]
            summarized_contents = self.rag_service.summarize_comments(contents_str)
            


            
            # ìš”ì•½ ë‚´ìš©ì„ defaultdictì— ì¶”ê°€ & DB ì €ì¥
            comments_to_save = []
            for content in summarized_contents:
                summarized_comment_obj = Comment(
                    comment_type=emotion,
                    content=content,
                    report_id=comments[0].report_id
                )
                summarized_comments[emotion].append(summarized_comment_obj)
                # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                comments_to_save.append({
                    "comment_type": emotion,
                    "content": content,
                    "report_id": comments[0].report_id
                })
            await self.comment_repository.save_bulk(comments_to_save)
            logger.info("ëŒ“ê¸€ ê²°ê³¼ë¥¼ PostgreSQL DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        summarize_and_save_time = time.time() - summarize_and_save_start
        logger.info(f"ğŸ“ ëŒ“ê¸€ ê°ì •ë³„ ìš”ì•½ ë° ì €ì¥ ì™„ë£Œ ({summarize_and_save_time:.2f}ì´ˆ)")
        return summarized_comments

    async def classify_comment_with_llm(self, comment: Comment) -> Comment:
        result = self.rag_service.classify_comment(comment.content)
        # commentì˜ comment_type ì—…ë°ì´íŠ¸
        comment.comment_type = result["comment_type"]
        # db ì €ì¥ í›„ ë°˜í™˜ -> ê·¸ëƒ¥ ë°˜í™˜
        return comment

    # ëŒ“ê¸€ì„ ë¶„ë¥˜
    def sample_comments(self, comments: list[Comment], threshold: int = 200, sample_rate: float = 0.1) -> tuple[list[Comment], bool]:
        """
        ëŒ“ê¸€ì´ threshold ì´ìƒì¼ ë•Œ sample_rate ë¹„ìœ¨ë¡œ ìƒ˜í”Œë§
        
        Args:
            comments: ì „ì²´ ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸
            threshold: ìƒ˜í”Œë§ ì‹œì‘ ê¸°ì¤€ (ê¸°ë³¸ 200ê°œ)
            sample_rate: ìƒ˜í”Œë§ ë¹„ìœ¨ (ê¸°ë³¸ 10%)
            
        Returns:
            (ìƒ˜í”Œë§ëœ ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸, ìƒ˜í”Œë§ ì—¬ë¶€ í”Œë˜ê·¸)
        """
        total_count = len(comments)
        
        if total_count < threshold:
            # ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ì „ì²´ ëŒ“ê¸€ ë°˜í™˜
            logger.info(f"ëŒ“ê¸€ ìˆ˜({total_count})ê°€ ì„ê³„ê°’({threshold}) ë¯¸ë§Œ, ì „ì²´ ëŒ“ê¸€ ë¶„ì„")
            return comments, False
        
        # ìƒ˜í”Œë§ ìˆ˜ ê³„ì‚° (ìµœì†Œ 20ê°œ ë³´ì¥)
        sample_size = max(20, int(total_count * sample_rate))
        sample_size = min(sample_size, total_count)  # ì „ì²´ ëŒ“ê¸€ ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡
        
        # ëœë¤ ìƒ˜í”Œë§
        sampled_comments = random.sample(comments, sample_size)
        
        logger.info(f"ëŒ“ê¸€ ìƒ˜í”Œë§: {total_count}ê°œ ì¤‘ {sample_size}ê°œ ì„ íƒ ({sample_rate*100:.0f}%)")
        return sampled_comments, True

    async def gather_classified_comments(self, comments: list[Comment])->DefaultDict[CommentType, list[Comment]]:
        grouped = defaultdict(list)
        for comment in comments:
            result = await self.classify_comment_with_llm(comment)
            grouped[result.comment_type].append(result)

        return grouped
    
    async def gather_classified_comments_optimized(self, all_comments: list[Comment]) -> tuple[DefaultDict[CommentType, list[Comment]], DefaultDict[CommentType, list[Comment]]]:
        """
        ìµœì í™”ëœ ëŒ“ê¸€ ê°ì • ë¶„ë¥˜ - ìƒ˜í”Œë§ì„ í†µí•œ ì„±ëŠ¥ ê°œì„ 
        
        Args:
            all_comments: ì „ì²´ ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ì „ì²´ ê°ì •ë³„ ë¶„ë¥˜ ë”•ì…”ë„ˆë¦¬, ìƒ˜í”Œë§ëœ ëŒ“ê¸€ë§Œì˜ ê°ì •ë³„ ë”•ì…”ë„ˆë¦¬)
        """
        # 1. ìƒ˜í”Œë§ ìˆ˜í–‰
        sampled_comments, is_sampled = self.sample_comments(all_comments)
        
        # 2. ìƒ˜í”Œë§ëœ ëŒ“ê¸€ë§Œ LLMìœ¼ë¡œ ê°ì • ë¶„ë¥˜
        llm_classify_start = time.time()
        logger.info(f"ğŸ¤– LLM ê°ì • ë¶„ë¥˜ ì‹œì‘: {len(sampled_comments)}ê°œ ëŒ“ê¸€")
        sample_grouped = defaultdict(list)
        for comment in sampled_comments:
            result = await self.classify_comment_with_llm(comment)
            sample_grouped[result.comment_type].append(result)
        llm_classify_time = time.time() - llm_classify_start
        logger.info(f"ğŸ¤– LLM ê°ì • ë¶„ë¥˜ ì™„ë£Œ ({llm_classify_time:.2f}ì´ˆ)")
        
        # 3. ìƒ˜í”Œë§í•˜ì§€ ì•Šì€ ê²½ìš° ê°™ì€ ë°ì´í„° ë‘ ë²ˆ ë°˜í™˜
        if not is_sampled:
            return sample_grouped, sample_grouped
        
        # 4. ìƒ˜í”Œë§í•œ ê²½ìš°: ê°ì • ë¶„í¬ ê³„ì‚°
        total_sampled = len(sampled_comments)
        emotion_distribution = {}
        for emotion, comments in sample_grouped.items():
            emotion_distribution[emotion] = len(comments) / total_sampled
        
        logger.info(f"ìƒ˜í”Œ ê°ì • ë¶„í¬: {dict(emotion_distribution)}")
        
        # 5. ì „ì²´ ëŒ“ê¸€ì— ê°ì • ë¶„í¬ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì ìš©
        final_grouped = defaultdict(list)
        unsampled_comments = [c for c in all_comments if c not in sampled_comments]
        
        # ìƒ˜í”Œë§ëœ ëŒ“ê¸€ì€ ì´ë¯¸ ë¶„ë¥˜ëœ ê°ì • ê·¸ëŒ€ë¡œ ì¶”ê°€
        for emotion, comments in sample_grouped.items():
            final_grouped[emotion].extend(comments)
        
        # ë‚˜ë¨¸ì§€ ëŒ“ê¸€ì— í™•ë¥ ì ìœ¼ë¡œ ê°ì • í• ë‹¹
        for comment in unsampled_comments:
            # ê°€ì¤‘ì¹˜ ëœë¤ ì„ íƒìœ¼ë¡œ ê°ì • í• ë‹¹
            emotions = list(emotion_distribution.keys())
            weights = list(emotion_distribution.values())
            
            if emotions and weights:
                assigned_emotion = random.choices(emotions, weights=weights, k=1)[0]
                comment.comment_type = assigned_emotion
                final_grouped[assigned_emotion].append(comment)
            else:
                # ë¶„í¬ê°€ ì—†ìœ¼ë©´ ì¤‘ë¦½ìœ¼ë¡œ í• ë‹¹
                comment.comment_type = CommentType.NEUTRAL
                final_grouped[CommentType.NEUTRAL].append(comment)
        
        # ìµœì¢… ë¶„í¬ ë¡œê¹…
        final_distribution = {emotion: len(comments) for emotion, comments in final_grouped.items()}
        logger.info(f"ìµœì¢… ê°ì • ë¶„í¬: {final_distribution}")
        
        # ì „ì²´ ë¶„ë¥˜ ê²°ê³¼ì™€ ìƒ˜í”Œë§ëœ ëŒ“ê¸€ë§Œ ë”°ë¡œ ë°˜í™˜
        return final_grouped, sample_grouped

    async def analyze_comments(self, video: Video, report_id: int) -> bool:
        """
        ì˜ìƒì˜ ëŒ“ê¸€ì„ ë¶„ì„í•˜ê³  ê°ì •ë³„ë¡œ ìš”ì•½í•˜ì—¬ ì €ì¥
        
        Args:
            video: ë¹„ë””ì˜¤ ê°ì²´
            report_id: ë¦¬í¬íŠ¸ ID
            
        Returns:
            ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False
        """
        start_time = time.time()
        logger.info(f"ğŸ’¬ ëŒ“ê¸€ ë¶„ì„ ì‹œì‘ - Report ID: {report_id}")
        
        try:
            # ìœ íŠœë¸Œ ì˜ìƒ ì•„ì´ë”” ì¡°íšŒ
            youtube_video_id = getattr(video, "youtube_video_id", None)
            if not youtube_video_id:
                logger.error("YouTube ì˜ìƒ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ëŒ“ê¸€ ì •ë³´ ì¡°íšŒ (YouTube API)
            api_start = time.time()
            comments_by_youtube = await self.youtube_comment_service.get_comments(youtube_video_id, report_id)
            api_time = time.time() - api_start
            logger.info(f"ğŸ’¬ YouTube ëŒ“ê¸€ API í˜¸ì¶œ ì™„ë£Œ ({api_time:.2f}ì´ˆ) - {len(comments_by_youtube)}ê°œ ëŒ“ê¸€")
            
            # Comment ê°ì²´ë¡œ ë³€í™˜
            comments_obj = await self.convert_to_comment_objects(comments_by_youtube)
            
            # ìµœì í™”ëœ ê°ì • ë¶„ë¥˜ ì‚¬ìš© (LLM API í˜¸ì¶œ)
            logger.info(f"ğŸ§  ì´ {len(comments_obj)}ê°œ ëŒ“ê¸€ ê°ì • ë¶„ë¥˜ ì‹œì‘")
            all_classified_result, sampled_result = await self.gather_classified_comments_optimized(comments_obj)
            
            # ì „ì²´ ëŒ“ê¸€ ê°œìˆ˜ ì €ì¥ (ìŠ¤ì¼€ì¼ë§ëœ ì „ì²´ ê°œìˆ˜)
            total_count_dict = {comment_type: len(comments) for comment_type, comments in all_classified_result.items()}
            logger.info(f"ì „ì²´ ëŒ“ê¸€ ê°ì • ë¶„í¬ (ìŠ¤ì¼€ì¼ë§ í¬í•¨): {total_count_dict}")
            
            # ê°ì •ë³„ ìš”ì•½ ìƒì„± (ìƒ˜í”Œë§ëœ ëŒ“ê¸€ë§Œ ì‚¬ìš© - ì •í™•í•œ ê°ì • ë¶„ë¥˜ ë³´ì¥)
            summary_start = time.time()
            logger.info(f"ğŸ“ ìš”ì•½ ìƒì„±ì— ì‚¬ìš©í•  ìƒ˜í”Œë§ëœ ëŒ“ê¸€: {sum(len(c) for c in sampled_result.values())}ê°œ")
            summarized_comments = await self.summarize_comments_by_emotions_with_llm(sampled_result)
            summary_time = time.time() - summary_start
            logger.info(f"ğŸ“ ëŒ“ê¸€ ìš”ì•½ ìƒì„± ì™„ë£Œ ({summary_time:.2f}ì´ˆ)")
            
            # ê°ì •ë³„ ëŒ“ê¸€ ê°œìˆ˜ ì—…ë°ì´íŠ¸ (ì „ì²´ ê°œìˆ˜ ì‚¬ìš©)
            db_start = time.time()
            count_dict = total_count_dict
            await self.report_repository.update_count(report_id, count_dict)
            db_time = time.time() - db_start
            logger.info(f"ğŸ—„ï¸ ëŒ“ê¸€ ê°œìˆ˜ DB ì €ì¥ ì™„ë£Œ ({db_time:.2f}ì´ˆ)")
            
            total_time = time.time() - start_time
            logger.info(f"ğŸ’¬ ëŒ“ê¸€ ë¶„ì„ ì „ì²´ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            total_time = time.time() - start_time
            logger.error(f"ğŸ’¬ ëŒ“ê¸€ ë¶„ì„ ì‹¤íŒ¨ ({total_time:.2f}ì´ˆ): {e}")
            raise

    # ìœ íŠœë¸Œ api ì—ì„œ ê°€ì ¸ì˜¨ ëŒ“ê¸€ì„ Comment ê°ì²´ë¡œ ë³€í™˜
    async def convert_to_comment_objects(self, comments: list[dict]) -> list[Comment]:
        comment_objects = []

        for data in comments:
            comment = Comment(
                comment_type=data.get("comment_type", None),
                content=data.get("content", ""),
                report_id=data.get("report_id"),
            )
            comment_objects.append(comment)
        return comment_objects
