from typing import DefaultDict, List, Any
import logging
import json
import asyncio
import time
from domain.comment.model.comment import Comment
from domain.report.repository.report_repository import ReportRepository
from domain.content_chunk.repository.content_chunk_repository import ContentChunkRepository
from domain.trend_keyword.repository.trend_keyword_repository import TrendKeywordRepository
from domain.trend_keyword.model.trend_keyword_type import TrendKeywordType
from domain.channel.repository.channel_repository import ChannelRepository
from domain.video.model.video import Video
from domain.channel.model.channel import Channel
from core.enums.source_type import SourceTypeEnum
from external.rag.rag_service_impl import RagServiceImpl
from external.rag import leave_analyize

logger = logging.getLogger(__name__)

class ReportService:
    def __init__(self):
        self.report_repository = ReportRepository()
        self.content_chunk_repository = ContentChunkRepository()
        self.trend_keyword_repository = TrendKeywordRepository()
        self.channel_repository = ChannelRepository()
        self.rag_service = RagServiceImpl()

    async def create_summary(self, video: Video, report_id: int, skip_vector_save: bool = False) -> bool:
        """
        ì˜ìƒ ìš”ì•½ì„ ìƒì„±í•˜ê³  Vector DBì™€ PostgreSQLì— ì €ì¥
        
        Args:
            video: ë¹„ë””ì˜¤ ê°ì²´
            report_id: ë¦¬í¬íŠ¸ ID
            skip_vector_save: Vector DB ì €ì¥ ìŠ¤í‚µ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            
        Returns:
            ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False
        """
        start_time = time.time()
        logger.info(f"ğŸ“„ ìš”ì•½ ìƒì„± ì‹œì‘ - Report ID: {report_id}")
        
        try:
            # ìœ íŠœë¸Œ ì˜ìƒ ì•„ì´ë”” ì¡°íšŒ
            youtube_video_id = getattr(video, "youtube_video_id", None)
            if not youtube_video_id:
                logger.error("YouTube ì˜ìƒ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ìš”ì•½ ìƒì„± (LLM API í˜¸ì¶œ)
            summary_start = time.time()
            summary = self.rag_service.summarize_video(youtube_video_id)
            summary_time = time.time() - summary_start
            logger.info(f"ğŸ¤– LLM API ìš”ì•½ ìƒì„± ì™„ë£Œ ({summary_time:.2f}ì´ˆ)")
            logger.info("ìš”ì•½ ê²°ê³¼:\n%s", summary)
            
            # ë²¡í„° DBì— ì €ì¥ (skip_vector_saveê°€ Falseì¸ ê²½ìš°ë§Œ)
            if not skip_vector_save:
                await self.content_chunk_repository.save_context(
                    source_type=SourceTypeEnum.VIDEO_SUMMARY,
                    source_id=report_id,
                    context=summary
                )
                logger.info("ìš”ì•½ ê²°ê³¼ë¥¼ ë²¡í„° DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            else:
                logger.info("[V2] ë²¡í„° DB ì €ì¥ì„ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤.")
            
            # PostgreSQLì— ì €ì¥
            pg_start = time.time()
            await self.report_repository.save({
                "id": report_id,
                "summary": summary,
                "title": video.title
            })
            pg_time = time.time() - pg_start
            logger.info(f"ğŸ—„ï¸ PostgreSQL DB ì €ì¥ ì™„ë£Œ ({pg_time:.2f}ì´ˆ)")
            
            total_time = time.time() - start_time
            logger.info(f"ğŸ“„ ìš”ì•½ ìƒì„± ì „ì²´ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            total_time = time.time() - start_time
            logger.error(f"ğŸ“„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨ ({total_time:.2f}ì´ˆ): {e}")
            raise

    async def analyze_viewer_retention(self, video: Video, report_id: int, token: str, skip_vector_save: bool = False) -> bool:
        """
        ì‹œì²­ì ì´íƒˆ ë¶„ì„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        
        Args:
            video: ë¹„ë””ì˜¤ ê°ì²´
            report_id: ë¦¬í¬íŠ¸ ID
            token: Google ì•¡ì„¸ìŠ¤ í† í°
            skip_vector_save: Vector DB ì €ì¥ ìŠ¤í‚µ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            
        Returns:
            ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False
        """
        start_time = time.time()
        logger.info(f"ğŸ“Š ì‹œì²­ì ì´íƒˆ ë¶„ì„ ì‹œì‘ - Report ID: {report_id}")
        
        try:
            leave_result = None
            max_retries = 3
            retry_count = 0
            
            # ì¬ì‹œë„ ë¡œì§ (ì´íƒˆ ë¶„ì„ API í˜¸ì¶œ)
            api_start = time.time()
            while retry_count < max_retries:
                try:
                    leave_result = await leave_analyize.analyze_leave(video, token)
                    api_time = time.time() - api_start
                    logger.info(f"ğŸ“ˆ ì´íƒˆ ë¶„ì„ API í˜¸ì¶œ ì™„ë£Œ ({api_time:.2f}ì´ˆ)")
                    break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                    
                except (AttributeError, TypeError, KeyError):
                    # ì¦‰ì‹œ ì‹¤íŒ¨í•´ì•¼ í•˜ëŠ” ì—ëŸ¬ë“¤
                    raise
                    
                except Exception as e:
                    error_type = e.__class__.__name__
                    
                    # ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„
                    if error_type in ['ConnectTimeout', 'ReadTimeout', 'ConnectionError', 'TimeoutError']:
                        retry_count += 1
                        if retry_count < max_retries:
                            wait_time = retry_count * 5  # ì§€ìˆ˜ ë°±ì˜¤í”„
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            # ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                            leave_result = "ì‹œì²­ì ì´íƒˆ ë¶„ì„ ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ)"
                            break
                    else:
                        # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ê°€ ì•„ë‹Œ ê²½ìš° ì¦‰ì‹œ ì¢…ë£Œ
                        raise
            
            # Vector DBì— ì €ì¥ (skip_vector_saveê°€ Falseì¸ ê²½ìš°ë§Œ)
            if not skip_vector_save:
                await self.content_chunk_repository.save_context(
                    source_type=SourceTypeEnum.VIEWER_ESCAPE_ANALYSIS,
                    source_id=report_id,
                    context=leave_result
                )
            else:
                logger.info("[V2] ë²¡í„° DB ì €ì¥ì„ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤.")
            
            # PostgreSQLì— ì €ì¥
            await self.report_repository.save({
                "id": report_id,
                "leave_analyze": leave_result
            })
            
            return True
            
        except Exception as e:
            raise

    async def analyze_optimization(self, video: Video, report_id: int, skip_vector_save: bool = False) -> bool:
        """
        ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„
        
        Args:
            video: ë¹„ë””ì˜¤ ê°ì²´
            report_id: ë¦¬í¬íŠ¸ ID
            skip_vector_save: Vector DB ì €ì¥ ìŠ¤í‚µ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            
        Returns:
            ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False
        """
        start_time = time.time()
        logger.info(f"âš™ï¸ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„ ì‹œì‘ - Report ID: {report_id}")
        
        try:
            # ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„ (LLM API í˜¸ì¶œ)
            opt_start = time.time()
            analyze_opt = await self.rag_service.analyze_algorithm_optimization(video_id=video.youtube_video_id, skip_vector_save=skip_vector_save)
            opt_time = time.time() - opt_start
            logger.info(f"âš™ï¸ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” LLM ë¶„ì„ ì™„ë£Œ ({opt_time:.2f}ì´ˆ)")
            
            # Vector DBì— ì €ì¥ (skip_vector_saveê°€ Falseì¸ ê²½ìš°ë§Œ)
            if not skip_vector_save:
                await self.content_chunk_repository.save_context(
                    source_type=SourceTypeEnum.ALGORITHM_OPTIMIZATION,
                    source_id=report_id,
                    context=analyze_opt
                )
            else:
                logger.info("[V2] ë²¡í„° DB ì €ì¥ì„ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤.")
            
            # PostgreSQLì— ì €ì¥
            pg_start = time.time()
            await self.report_repository.save({
                "id": report_id,
                "optimization": analyze_opt
            })
            pg_time = time.time() - pg_start
            logger.info(f"ğŸ—„ï¸ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„ PostgreSQL DB ì €ì¥ ì™„ë£Œ ({pg_time:.2f}ì´ˆ)")
            
            total_time = time.time() - start_time
            logger.info(f"âš™ï¸ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„ ì „ì²´ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            total_time = time.time() - start_time
            logger.error(f"âš™ï¸ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„ ì‹¤íŒ¨ ({total_time:.2f}ì´ˆ): {e}")
            raise

    async def analyze_trends_and_save(self, video: Video, report_id: int, skip_vector_save: bool = False) -> bool:
        """
        íŠ¸ë Œë“œ ë¶„ì„ ë° í‚¤ì›Œë“œ ì €ì¥
        
        Args:
            video: ë¹„ë””ì˜¤ ê°ì²´
            report_id: ë¦¬í¬íŠ¸ ID
            skip_vector_save: Vector DB ì €ì¥ ìŠ¤í‚µ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            
        Returns:
            ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ False
        """
        start_time = time.time()
        logger.info(f"ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘ - Report ID: {report_id}")
        
        try:
            # 1. ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„
            realtime_keyword = self.rag_service.analyze_realtime_trends()
            
            # 2. ì±„ë„ ì •ë³´ ì¡°íšŒ
            channel_id = getattr(video, "channel_id", None)
            if not channel_id:
                raise ValueError("videoì— channel_idê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            channel = await self.channel_repository.find_by_id(channel_id)
            if not channel:
                raise ValueError(f"channel_id={channel_id}ì— í•´ë‹¹í•˜ëŠ” ì±„ë„ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # 3. ì±„ë„ ë§ì¶¤í˜• íŠ¸ë Œë“œ ë¶„ì„
            channel_concept = getattr(channel, "concept", "")
            target_audience = getattr(channel, "target", "")
            
            channel_keyword = self.rag_service.analyze_channel_trends(
                channel_concept=channel_concept,
                target_audience=target_audience
            )
            
            # 4. Vector DBì— ì±„ë„ ë§ì¶¤í˜• í‚¤ì›Œë“œ ì €ì¥ (skip_vector_saveê°€ Falseì¸ ê²½ìš°ë§Œ)
            if not skip_vector_save:
                await self.content_chunk_repository.save_context(
                    source_type=SourceTypeEnum.PERSONALIZED_KEYWORDS,
                    source_id=report_id,
                    context=json.dumps(channel_keyword, ensure_ascii=False)
                )
                logger.info("ì±„ë„ ë§ì¶¤í˜• í‚¤ì›Œë“œë¥¼ Vector DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            else:
                logger.info("[V2] ë²¡í„° DB ì €ì¥ì„ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤.")
            
            # 5. PostgreSQLì— í‚¤ì›Œë“œ ì €ì¥
            # ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì €ì¥
            if realtime_keyword and "trends" in realtime_keyword:
                realtime_keywords_to_save = []
                for keyword_data in realtime_keyword["trends"]:
                    trend_keyword = {
                        "report_id": report_id,
                        "keyword_type": TrendKeywordType.REAL_TIME,
                        "keyword": keyword_data.get("keyword", ""),
                        "score": keyword_data.get("score", 0)
                    }
                    realtime_keywords_to_save.append(trend_keyword)
                
                await self.trend_keyword_repository.save_bulk(realtime_keywords_to_save)
                logger.info("ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ PostgreSQL DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

            # ì±„ë„ ë§ì¶¤í˜• í‚¤ì›Œë“œ ì €ì¥
            if channel_keyword and "customized_trends" in channel_keyword:
                channel_keywords_to_save = []
                for keyword_data in channel_keyword["customized_trends"]:
                    trend_keyword = {
                        "report_id": report_id,
                        "keyword_type": TrendKeywordType.CHANNEL,
                        "keyword": keyword_data.get("keyword", ""),
                        "score": keyword_data.get("score", 0)
                    }
                    channel_keywords_to_save.append(trend_keyword)
                
                await self.trend_keyword_repository.save_bulk(channel_keywords_to_save)
                logger.info("ì±„ë„ ë§ì¶¤í˜• í‚¤ì›Œë“œë¥¼ PostgreSQL DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            
            total_time = time.time() - start_time
            logger.info(f"ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„ ì „ì²´ ì™„ë£Œ ({total_time:.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            total_time = time.time() - start_time
            logger.error(f"ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨ ({total_time:.2f}ì´ˆ): {e}")
            raise

    async def update_report_emotion_counts(self, report_id: int, comment_dict:DefaultDict[str,List[Comment]]) -> bool:
        """
        ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        count_dict = {comment_type: len(comments) for comment_type, comments in comment_dict.items()}
        logger.info("ëŒ“ê¸€ ê°œìˆ˜ë¥¼ PostgreSQL DBì— ì €ì¥í•©ë‹ˆë‹¤.")
        return await self.report_repository.update_count(report_id, count_dict)