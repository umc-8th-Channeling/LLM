from typing import List

from dotenv import load_dotenv
from domain.content_chunk.repository.content_chunk_repository import ContentChunkRepository
import external.youtube.analytics_service as analyticsServcie
import external.rag.chunk_service as ChunkService # â† ì§€ì—° importë¡œ ìˆœí™˜ì°¸ì¡° ë°©ì§€
from external.youtube.transcript_service import TranscriptService  # ìœ íŠœë¸Œ ìë§‰ ì²˜ë¦¬ ì„œë¹„ìŠ¤
from domain.video.model.video import Video
from core.enums.source_type import SourceTypeEnum
from external.rag.rag_service_impl import RagServiceImpl
import json
from core.llm.prompt_template_manager import PromptTemplateManager
from domain.channel.repository.channel_repository import ChannelRepository
import os
import logging
import time
load_dotenv()

logger = logging.getLogger(__name__)
transcript_service = TranscriptService()
content_repository = ContentChunkRepository()
rag_service = RagServiceImpl()
channel_repository = ChannelRepository()

async def analyze_leave(video: Video, token: str) -> str:
    try:
        logger.info(f"ì‹œì²­ì ì´íƒˆ ë¶„ì„ ì‹œì‘ - ë¹„ë””ì˜¤ ID: {video.id}, ìœ íŠœë¸Œ ID: {video.youtube_video_id}")
    






        # 1. ì˜ìƒ, ì±„ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        
        # ì˜ìƒ ê°€ì ¸ì˜¤ê¸°
        youtube_video_id = video.youtube_video_id
        video_id = video.id
        logger.info(f"ë¶„ì„ ëŒ€ìƒ - ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ID: {youtube_video_id}, ë‚´ë¶€ ë¹„ë””ì˜¤ ID: {video_id}")

        # ì±„ë„ ê°€ì ¸ì˜¤ê¸°    
        channel_id = video.channel_id
        channel = await channel_repository.find_by_id(channel_id)
        if not channel:
            logger.error(f"ì±„ë„ ID {channel_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            raise ValueError(f"ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {channel_id}")

    # context, analytics_data = await asyncio.gather(
    #     transcript_service.get_structured_transcript(youtube_video_id),
    #     analyticsServcie.get_youtube_analytics_data(token, youtube_video_id)
    # )




        # 2. ì˜ìƒì˜ ìŠ¤í¬ë¦½íŠ¸ ê°€ì ¸ì˜¤ê¸°
        # ëŒ€ë³¸ ìŠ¤í¬ë¦½íŠ¸ ê°€ì ¸ì˜¤ê¸°
        transcript_start = time.time()
        logger.info("ğŸ“œ ì˜ìƒ ìë§‰ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        context = transcript_service.get_structured_transcript(youtube_video_id)
        transcript_time = time.time() - transcript_start
        logger.info(f"ğŸ“œ ìë§‰ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ ({transcript_time:.2f}ì´ˆ)")
        
        if not context:
            logger.error("ìë§‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "ìë§‰ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ëŠ” ì˜ìƒì…ë‹ˆë‹¤."
        
        # 3. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•´ì„œ ì˜ìƒ ì´ ê¸¸ì´ êµ¬í•˜ê¸°
        video_length = context[-1]["end_time"]
        logger.info(f"ì˜ìƒ ì´ ê¸¸ì´: {video_length}ì´ˆ")

        # 4. ì˜ìƒ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (analytics)
        analytics_start = time.time()
        logger.info("ğŸ“Š YouTube Analytics ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        metrics = "audienceWatchRatio,relativeRetentionPerformance"
        dimensions = "elapsedVideoTimeRatio"
        analytics_data = await analyticsServcie.get_youtube_analytics_data(token, youtube_video_id, metrics, dimensions)
        analytics_time = time.time() - analytics_start
        logger.info(f"ğŸ“Š YouTube Analytics ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ ({analytics_time:.2f}ì´ˆ)")
        
        if not analytics_data or "rows" not in analytics_data:
            logger.warning("Analytics ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©.")
            analytics_data = {"rows": []}

        # 5. ë¶„ì„ ê²°ê³¼ë¡œ ì´íƒˆ ì‹œì  êµ¬í•˜ê¸°
        worst_ratio = analyticsServcie.find_max_drop_time(analytics_data.get("rows", []))
        logger.info(f"ìµœëŒ€ ì´íƒˆ ì‹œì  ë¹„ìœ¨: {worst_ratio}")

        # 6. ì‹œê°„ ë‹¨ìœ„ ì²­í‚¹ ë° ì„ë² ë”© ì €ì¥
        chunking_start = time.time()
        logger.info("ğŸ”§ ì‹œê°„ ë‹¨ìœ„ ì²­í‚¹ ë°ì´í„° í™•ì¸ ì¤‘...")
        exists = await content_repository.exists_by_chunk_type_and_id("time", str(video_id))
        if exists:
            logger.info("ê¸°ì¡´ì— ì €ì¥í•œ ì  ìˆëŠ” ì˜ìƒì…ë‹ˆë‹¤. ëŒ€ë³¸ ê¸°ë°˜ì˜ ì²­í‚¹ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            logger.info("ğŸ”§ ì‹œê°„ ë‹¨ìœ„ ì²­í‚¹ ìƒì„± ì¤‘...")
            await ChunkService.create_time_chunks_with_focus(video_id, video_length, context, analytics_data.get("rows", []), worst_ratio)
        
        # 7. ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹ ë° ì„ë² ë”© ì €ì¥
        logger.info("ğŸ”§ ì˜ë¯¸ ë‹¨ìœ„ ì²­í‚¹ ìƒì„± ì¤‘...")
        await ChunkService.create_meaning_chunks_with_focus(video_id, video_length, context, analytics_data.get("rows", []), worst_ratio)
        chunking_time = time.time() - chunking_start
        logger.info(f"ğŸ”§ ì²­í‚¹ ë° ì„ë² ë”© ì €ì¥ ì™„ë£Œ ({chunking_time:.2f}ì´ˆ)")



        # 8. ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ì™€ ìœ ì‚¬ë„ ë¶„ì„í•´ì„œ, ê° ì§ˆë¬¸ë§ˆë‹¤ 3ê°œì”©ì˜ ì²­í‚¹ì„ ì¡°íšŒ
        similarity_start = time.time()
        logger.info("ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì‘...")
        
        # # 1) ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        questions = {
            "cause": "ì´ ì˜ìƒì˜ ì‹œì²­ ì´íƒˆ ì›ì¸ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.",
            "improvement": "ì´ ì˜ìƒì˜ ì‹œì²­ ì´íƒˆì„ ì¤„ì´ê¸° ìœ„í•œ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ ì£¼ì„¸ìš”.",
            "editing_flow": "ì´ ì˜ìƒì— ì í•©í•œ ì˜ˆìƒ í¸ì§‘ íë¦„ì„ ì œì•ˆí•´ ì£¼ì„¸ìš”."
        }

        # ì¶”ê°€ í•„í„°ë§ì´ ìˆë‹¤ë©´..?
        meta = {}
        # 2) ì§ˆë¬¸ë³„ ì„ë² ë”© ë° ìœ ì‚¬ ì²­í‚¹ ê²€ìƒ‰
        # ì´íƒˆ ì›ì¸ ì§ˆë¬¸ì˜ ìƒìœ„ 3ê°œ ì²­í‚¹ ë°ì´í„° ì¡°íšŒ 
        cause_chunk = await content_repository.search_similar_K(questions["cause"],SourceTypeEnum.VIEWER_ESCAPE_ANALYSIS.value.upper(),str(video_id),meta ,3)
        # ì´íƒˆ ì›ì¸ ì§ˆë¬¸ì˜ ìƒìœ„ 3ê°œ ì²­í‚¹ ë°ì´í„° ì¡°íšŒ 
        improvement_chunk = await content_repository.search_similar_K(questions["improvement"],SourceTypeEnum.VIEWER_ESCAPE_ANALYSIS.value.upper(),str(video_id),meta ,3)
        # ì´íƒˆ ì›ì¸ ì§ˆë¬¸ì˜ ìƒìœ„ 3ê°œ ì²­í‚¹ ë°ì´í„° ì¡°íšŒ 
        editing_flow_chunk = await content_repository.search_similar_K(questions["editing_flow"],SourceTypeEnum.VIEWER_ESCAPE_ANALYSIS.value.upper(),str(video_id),meta ,3)
        
        similarity_time = time.time() - similarity_start
        logger.info(f"ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ ({similarity_time:.2f}ì´ˆ)")


        # llm ì¶œë ¥
        worst_sec = int(worst_ratio * video_length)  # ìµœì•… ì´íƒˆ ì‹œì (ì´ˆ)
        focus_range_sec = max(10, min(int(0.04 * video_length), 300))  # ì§‘ì¤‘ ë²”ìœ„: ì˜ìƒ ê¸¸ì´ì˜ 4%, ìµœì†Œ 10ì´ˆ, ìµœëŒ€ 5ë¶„
        start_focus_time = max(0, worst_sec - focus_range_sec // 2)
        end_focus_time = min(video_length, worst_sec + focus_range_sec // 2)
        # 3. context ìƒì„±
        context_data = {
            "cause_chunk": json.dumps(cause_chunk, ensure_ascii=False, indent=2),
            "improvement_chunk": json.dumps(improvement_chunk, ensure_ascii=False, indent=2),
            "editing_flow_chunk": json.dumps(editing_flow_chunk, ensure_ascii=False, indent=2),
            "worst_sec": worst_sec,
            "start_focus_time": start_focus_time,
            "end_focus_time": end_focus_time,
            "video_length": video_length,
            "video_title" : video.title,
            "video_description" : video.description,
            "video_category" : video.video_category,
            "channel_concept" : channel.concept,
            "channel_target" : channel.target,
            "channel_hashtag" : channel.channel_hash_tag
        }

        # 9. 1,8ë²ˆì—ì„œ ì¡°íšŒí•œ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ë„£ê¸°   
        prompt_template_str = PromptTemplateManager.get_viewer_escape_analysis_prompt()
        formatted_prompt = prompt_template_str.format(**context_data)  
    
        # 10. LLM ì§ì ‘ í˜¸ì¶œí•´ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        llm_start = time.time()
        logger.info("ğŸ¤– LLM ì´íƒˆ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
        result = rag_service.execute_llm_direct(formatted_prompt)
        llm_time = time.time() - llm_start
        logger.info(f"ğŸ¤– LLM ì´íƒˆ ë¶„ì„ ì™„ë£Œ ({llm_time:.2f}ì´ˆ)")
        
        return result

    except Exception as e:
        logger.error(f"ì‹œì²­ì ì´íƒˆ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise e
   
